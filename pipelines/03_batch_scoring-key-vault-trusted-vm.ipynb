{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (c) Microsoft Corporation. All rights reserved.  \n",
    "Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "This demo notebook shows how to create an AML pipeline that:\n",
    "\n",
    "1. Copies input data from ADLS to blob\n",
    "1. Reads input blob data, scores, and writes output to blob\n",
    "1. Copies output data from blob to ADLS\n",
    "\n",
    "NOTE: This version uses [KeyVault](https://azure.microsoft.com/en-us/services/key-vault/) to store secrets for ADLS and blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "Make sure you go through the configuration Notebook located at https://github.com/Azure/MachineLearningNotebooks first if you haven't. This sets you up with a working config file that has information on your workspace, subscription id, etc.\n",
    "\n",
    "Next, authenticate your VM to Key Vault using MSI. The following steps are based on the tutorial: [Use a Windows VM system-assigned managed identity to access Azure Key Vault](\n",
    "https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/tutorial-windows-vm-access-nonaad)\n",
    "\n",
    "browse: Azure portal\n",
    "\n",
    "## create KeyVault\n",
    "add the following secrets:\n",
    "- account-key-blob: <your_storage_account_key\n",
    "- tenant-id: <your_tenant_id>\n",
    "- client-id: <your_client_id>\n",
    "- client-secret: <your_client_secret>\n",
    "\n",
    "# turn on MSI for your VM\n",
    "click: <your vm>\n",
    "click: identity\n",
    "click: System assigned: On (off by default)\n",
    "click: Save\n",
    "\n",
    "# grant VM access to secrets in your Key Vault\n",
    "click: <your key vault>\n",
    "click: Access policies > Add new\n",
    "select: Configure from template: Secret Management\n",
    "select: Select principal: <your vm>\n",
    "click: Select\n",
    "click: OK > Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Datastore\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.compute import AmlCompute, ComputeTarget, DataFactoryCompute\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.runconfig import CondaDependencies, RunConfiguration\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "from azureml.exceptions import ComputeTargetException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azureml.core import Workspace, Run, Experiment\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up machine learning resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create datastores\n",
    "\n",
    "ADLS will contain the input and output data. Blob will act as a \"staging\" area for the data, since it can be mounted from AML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "key_vault_name=\"<your_key_vault_name>\"\n",
    "\n",
    "# securely get key vault token\n",
    "headers={'Metadata': 'true'}\n",
    "url = \"http://169.254.169.254/metadata/identity/oauth2/token?api-version=2018-02-01&resource=https%3A%2F%2Fvault.azure.net\"\n",
    "response = requests.get(url=url, headers=headers).json()\n",
    "key_vault_access_token = response['access_token']\n",
    "\n",
    "# get secrets\n",
    "headers={'Authorization': 'Bearer ' + key_vault_access_token}\n",
    "\n",
    "url=f\"https://{key_vault_name}.vault.azure.net/secrets/account-key-blob?api-version=2016-10-01\"\n",
    "response = requests.get(url=url, headers=headers).json()\n",
    "account_key_blob=response['value']\n",
    "\n",
    "url=f\"https://{key_vault_name}.vault.azure.net/secrets/tenant-id?api-version=2016-10-01\"\n",
    "response = requests.get(url=url, headers=headers).json()\n",
    "tenant_id=response['value']\n",
    "\n",
    "url=f\"https://{key_vault_name}.vault.azure.net/secrets/client-id?api-version=2016-10-01\"\n",
    "response = requests.get(url=url, headers=headers).json()\n",
    "client_id=response['value']\n",
    "\n",
    "url=f\"https://{key_vault_name}.vault.azure.net/secrets/client-secret?api-version=2016-10-01\"\n",
    "response = requests.get(url=url, headers=headers).json()\n",
    "client_secret=response['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name_blob=\"MyBlobDatastore\"\n",
    "account_name_blob = \"<your_storage_account_name>\"\n",
    "container_name_blob=\"batchscoring\"\n",
    "\n",
    "blob_datastore = Datastore.register_azure_blob_container(ws, \n",
    "                      datastore_name=datastore_name_blob, \n",
    "                      container_name=container_name_blob, \n",
    "                      account_name=account_name_blob,\n",
    "                      account_key= account_key_blob,                                   \n",
    "                      overwrite=True)\n",
    "print(\"registered datastore with name: %s\" % datastore_name_blob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore_name_adls=\"MyAdlsDatastore\"\n",
    "store_name_adls=\"<your_adls_storage_account_name>\"\n",
    "\n",
    "adls_datastore = Datastore.register_azure_data_lake(\n",
    "        workspace=ws,\n",
    "        datastore_name=datastore_name_adls,\n",
    "        subscription_id=ws.subscription_id, # subscription id of ADLS account\n",
    "        resource_group=ws.resource_group, # resource group of ADLS account\n",
    "        store_name=store_name_adls, # ADLS account name\n",
    "        tenant_id=tenant_id, # tenant id of service principal\n",
    "        client_id=client_id, # client id of service principal\n",
    "        client_secret=client_secret) # the secret of service principal\n",
    "print(\"registered datastore with name: %s\" % datastore_name_adls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure data references\n",
    "Now you need to add references to the data, as inputs to the appropriate pipeline steps in your pipeline. A data source in a pipeline is represented by a DataReference object. The DataReference object points to data that lives in, or is accessible from, a datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://docs.microsoft.com/en-us/azure/machine-learning/service/reference-azure-machine-learning-cli\n",
    "\n",
    "input_dir_adls_dataref = DataReference(datastore=adls_datastore,\n",
    "                          data_reference_name=\"input_dir_adls\",\n",
    "                          path_on_datastore=\"input\",\n",
    "                          mode=\"download\")\n",
    "model_dir_adls_dataref = DataReference(datastore=adls_datastore,\n",
    "                          data_reference_name=\"model_dir_adls\",\n",
    "                          path_on_datastore=\"model\",\n",
    "                          mode=\"download\")\n",
    "output_dir_adls_dataref = DataReference(datastore=adls_datastore,\n",
    "                          data_reference_name=\"output_dir_adls\",\n",
    "                          path_on_datastore=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_blob_dataref = DataReference(datastore=blob_datastore, \n",
    "                             data_reference_name=\"input_dir_blob\",\n",
    "                             path_on_datastore=\"input_from_adls\")\n",
    "model_dir_blob_dataref = DataReference(datastore=blob_datastore, \n",
    "                          data_reference_name=\"model_dir_blob\",\n",
    "                          path_on_datastore=\"model_from_adls\")\n",
    "#output_dir_blob_dataref = DataReference(datastore=blob_datastore, \n",
    "#                          data_reference_name=\"output_dir_blob\",\n",
    "#                          path_on_datastore=\"output_to_adls\")\n",
    "output_dir_blob_pipedata = PipelineData(name=\"output\", # folder path\n",
    "                          datastore=blob_datastore, \n",
    "                          output_path_on_compute=\"output_to_adls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create and attach Compute targets\n",
    "Use the below code to create and attach Compute targets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# choose a name for your cluster\n",
    "aml_compute_name = os.environ.get(\"AML_COMPUTE_NAME\", \"gpu-cluster\")\n",
    "#aml_compute_name = os.environ.get(\"AML_COMPUTE_NAME\", \"cpu-cluster\")\n",
    "cluster_min_nodes = os.environ.get(\"AML_COMPUTE_MIN_NODES\", 0)\n",
    "cluster_max_nodes = os.environ.get(\"AML_COMPUTE_MAX_NODES\", 1)\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_SKU\", \"STANDARD_NC6\")\n",
    "#vm_size = os.environ.get(\"AML_COMPUTE_SKU\", \"STANDARD_D2_V2\")\n",
    "\n",
    "\n",
    "if aml_compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[aml_compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + aml_compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size, # NC6 is GPU-enabled\n",
    "                                                                vm_priority = 'lowpriority', # optional\n",
    "                                                                min_nodes = cluster_min_nodes, \n",
    "                                                                max_nodes = cluster_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, aml_compute_name, provisioning_config)\n",
    "    \n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "    \n",
    "     # For a more detailed view of current Azure Machine Learning Compute  status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a local model directory\n",
    "model_dir = 'models'\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data to local model directory\n",
    "\n",
    "#https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-data-operations-python\n",
    "#https://stackoverflow.com/questions/48208389/python-code-to-access-azure-data-lake-store#48213226\n",
    "!pip install azure-mgmt-resource\n",
    "!pip install azure-mgmt-datalake-store\n",
    "!pip install azure-datalake-store\n",
    "\n",
    "from azure.datalake.store import core, lib, multithread\n",
    "\n",
    "## Download a file\n",
    "token = lib.auth(tenant_id = tenant_id, client_secret = client_secret, client_id = client_id)\n",
    "adlsFileSystemClient = core.AzureDLFileSystem(token, store_name=store_name_adls)\n",
    "multithread.ADLDownloader(adlsFileSystemClient, \n",
    "                          #lpath=model_dir,\n",
    "                          lpath='models',\n",
    "                          rpath='model',\n",
    "                          nthreads=64, overwrite=True, buffersize=4194304, blocksize=4194304)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model with Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from azureml.core.model import Model\n",
    "\n",
    "# register downloaded model \n",
    "model = Model.register(model_path = \"models/\",\n",
    "                       model_name = \"keras\", # this is the name the model is registered as\n",
    "                       tags = {'pretrained': \"keras\"},\n",
    "                       description = \"Keras model used to score Indian dataset\",\n",
    "                       workspace = ws)\n",
    "# remove the downloaded dir after registration if you wish\n",
    "#shutil.rmtree(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your scoring script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the scoring, we use a batch scoring script `batch_scoring.py`, which is located in the same directory that this notebook is in. You can take a look at this script to see how you might modify it for your custom batch scoring task.\n",
    "\n",
    "The python script `batch_scoring.py` takes input data from blob, performs scoring, and writes the results back out to blob.\n",
    "\n",
    "The script `batch_scoring.py` takes the following parameters:\n",
    "\n",
    "- `--input_dir`  : the blob path containing the input data\n",
    "- `--output_dir` : the blob path containing the output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and run the batch scoring pipeline\n",
    "You have everything you need to build the pipeline. Let's put all these together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Specify the environment to run the script\n",
    "Specify the conda dependencies for your script. You will need this object when you create the pipeline step later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import DEFAULT_GPU_IMAGE\n",
    "\n",
    "#cd = CondaDependencies.create(pip_packages=[\"tensorflow-gpu==1.10.0\", \"azureml-defaults\"])\n",
    "cd = CondaDependencies.create(pip_packages=[\"tensorflow-gpu==1.10.0\", \"azureml-defaults\", \"keras\"])\n",
    "\n",
    "# Runconfig\n",
    "amlcompute_run_config = RunConfiguration(conda_dependencies=cd)\n",
    "amlcompute_run_config.environment.docker.enabled = True\n",
    "amlcompute_run_config.environment.docker.gpu_support = True\n",
    "amlcompute_run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "amlcompute_run_config.environment.spark.precache_packages = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the pipeline step\n",
    "Create the pipeline step using the script, environment configuration, and parameters. Specify the compute target you already attached to your workspace as the target of execution of the script. We will use PythonScriptStep to create the pipeline step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_factory_name = 'adftest'\n",
    "\n",
    "def get_or_create_data_factory(workspace, factory_name):\n",
    "    try:\n",
    "        return DataFactoryCompute(workspace, factory_name)\n",
    "    except ComputeTargetException as e:\n",
    "        if 'ComputeTargetNotFound' in e.message:\n",
    "            print('Data factory not found, creating...')\n",
    "            provisioning_config = DataFactoryCompute.provisioning_configuration()\n",
    "            data_factory = ComputeTarget.create(workspace, factory_name, provisioning_config)\n",
    "            data_factory.wait_for_completion()\n",
    "            return data_factory\n",
    "        else:\n",
    "            raise e\n",
    "            \n",
    "data_factory_compute = get_or_create_data_factory(ws, data_factory_name)\n",
    "\n",
    "print(\"setup data factory account complete\")\n",
    "\n",
    "# CLI:\n",
    "# Create: az ml computetarget setup datafactory -n <name>\n",
    "# BYOC: az ml computetarget attach datafactory -n <name> -i <resource-id>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_input_data_from_adls_to_blob = DataTransferStep(\n",
    "    name=\"transfer_adls_to_blob\",\n",
    "    source_data_reference=input_dir_adls_dataref,\n",
    "    destination_data_reference=input_dir_blob_dataref,\n",
    "    compute_target=data_factory_compute,\n",
    "    allow_reuse=False)\n",
    "\n",
    "print(\"data transfer step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_score_step_keras_registered = PythonScriptStep(\n",
    "    name=\"batch_scoring\",\n",
    "    script_name=\"batch_scoring.py\",\n",
    "    arguments=[\"--input_dir\", input_dir_blob_dataref,\n",
    "               \"--output_dir\", output_dir_blob_pipedata],\n",
    "    compute_target=compute_target,\n",
    "    inputs=[input_dir_blob_dataref],\n",
    "    outputs=[output_dir_blob_pipedata],\n",
    "    runconfig=amlcompute_run_config,\n",
    "    allow_reuse=False\n",
    ")\n",
    "print(\"python script step created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_output_data_from_blob_to_adls = DataTransferStep(\n",
    "    name=\"transfer_blob_to_adls\",\n",
    "    source_data_reference=output_dir_blob_pipedata,\n",
    "    destination_data_reference=output_dir_adls_dataref,\n",
    "    compute_target=data_factory_compute,\n",
    "    allow_reuse=False,\n",
    "    source_reference_type=\"directory\")\n",
    "\n",
    "print(\"data transfer step created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the pipeline\n",
    "At this point you can run the pipeline and examine the output it produced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[copy_input_data_from_adls_to_blob,\n",
    "                                         batch_score_step_keras_registered,\n",
    "                                         copy_output_data_from_blob_to_adls])\n",
    "pipeline_run = Experiment(ws, 'batch_scoring_keras').submit(pipeline, pipeline_params={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.wait_for_completion(show_output=True)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "hichando"
   }
  ],
  "kernelspec": {
   "display_name": "Python [conda env:AzureML]",
   "language": "python",
   "name": "conda-env-AzureML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
